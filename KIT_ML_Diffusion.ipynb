{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db58cf53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b3dc90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = torch.from_numpy(np.load(\"KIT-ML/Mean.npy\")).float()\n",
    "std  = torch.from_numpy(np.load(\"KIT-ML/Std.npy\")).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c7acc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KITMotionDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        root,\n",
    "        split=\"train\",\n",
    "        max_len=None,\n",
    "        normalize=True,\n",
    "        use_cache=True\n",
    "    ):\n",
    "        self.motion_dir = os.path.join(root, \"new_joint_vecs\")\n",
    "        self.text_dir = os.path.join(root, \"texts\")\n",
    "        split_file = os.path.join(root, f\"{split}.txt\")\n",
    "        \n",
    "        # Cache file path\n",
    "        cache_file = os.path.join(root, f\"cached_{split}_maxlen{max_len}_norm{normalize}.pt\")\n",
    "\n",
    "        # Try to load from cache\n",
    "        if use_cache and os.path.exists(cache_file):\n",
    "            print(f\"Loading preprocessed dataset from {cache_file}...\")\n",
    "            cached_data = torch.load(cache_file)\n",
    "            self.ids = cached_data['ids']\n",
    "            self.motions = cached_data['motions']\n",
    "            self.texts = cached_data['texts']\n",
    "            self.max_len = cached_data['max_len']\n",
    "            self.normalize = cached_data['normalize']\n",
    "            self.mean = cached_data.get('mean', None)\n",
    "            self.std = cached_data.get('std', None)\n",
    "            print(f\"Loaded {len(self.ids)} preprocessed motions from cache\")\n",
    "            return\n",
    "\n",
    "        # Otherwise, process from scratch\n",
    "        with open(split_file, \"r\") as f:\n",
    "            all_ids = [line.strip() for line in f]\n",
    "\n",
    "        # Filter out missing files\n",
    "        self.ids = []\n",
    "        missing_count = 0\n",
    "        for mid in all_ids:\n",
    "            path = os.path.join(self.motion_dir, f\"{mid}.npy\")\n",
    "            if os.path.exists(path):\n",
    "                self.ids.append(mid)\n",
    "            else:\n",
    "                missing_count += 1\n",
    "        \n",
    "        if missing_count > 0:\n",
    "            print(f\"Warning: {missing_count}/{len(all_ids)} motion files missing from {split} split\")\n",
    "        print(f\"Loaded {len(self.ids)} valid motions from {split} split\")\n",
    "\n",
    "        self.max_len = max_len\n",
    "        self.normalize = normalize\n",
    "\n",
    "        \n",
    "        # PRELOAD all motions and texts into memory\n",
    "        self.motions = {}\n",
    "        self.texts = {}\n",
    "\n",
    "        print(\"Preloading motions and texts into memory...\")\n",
    "        \n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        \n",
    "        for mid in self.ids:\n",
    "            # Load motion\n",
    "            motion_path = os.path.join(self.motion_dir, f\"{mid}.npy\")\n",
    "            motion = np.load(motion_path)  # (T, F)\n",
    "            \n",
    "            # Truncate\n",
    "            if self.max_len is not None:\n",
    "                motion = motion[:self.max_len]\n",
    "            \n",
    "            motion = torch.from_numpy(motion).float()  # (T, F)\n",
    "            \n",
    "            # Normalize\n",
    "            motion = (motion - self.mean) / (self.std + 1e-8)\n",
    "            \n",
    "            self.motions[mid] = motion\n",
    "            \n",
    "            # Load text description\n",
    "            text_path = os.path.join(self.text_dir, f\"{mid}.txt\")\n",
    "            if os.path.exists(text_path):\n",
    "                with open(text_path, \"r\") as f:\n",
    "                    text_line = f.read().strip()\n",
    "                    # Format: \"description#tokens#...\" - extract first part\n",
    "                    text = text_line.split(\"#\")[0]\n",
    "                    self.texts[mid] = text\n",
    "            else:\n",
    "                self.texts[mid] = \"a person is moving\"\n",
    "        print(f\"Preloaded {len(self.motions)} motions and {len(self.texts)} texts\")\n",
    "            \n",
    "        # Save to cache\n",
    "        if use_cache:\n",
    "            print(f\"Saving preprocessed dataset to {cache_file}...\")\n",
    "            torch.save({\n",
    "                'ids': self.ids,\n",
    "                'motions': self.motions,\n",
    "                'texts': self.texts,\n",
    "                'max_len': self.max_len,\n",
    "                'normalize': self.normalize,\n",
    "                'mean': self.mean,\n",
    "                'std': self.std\n",
    "            }, cache_file)\n",
    "            print(\"Cache saved!\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        mid = self.ids[idx]\n",
    "        motion = self.motions[mid]  # Already preprocessed!\n",
    "        text = self.texts[mid]\n",
    "\n",
    "        return {\n",
    "            \"motion\": motion,\n",
    "            \"length\": motion.shape[0],\n",
    "            \"text\": text,\n",
    "            \"id\": mid\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6686378a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_motion(batch):\n",
    "    motions = [b[\"motion\"] for b in batch]     # (T,F)\n",
    "    lengths = torch.tensor([b[\"length\"] for b in batch])\n",
    "    texts = [b[\"text\"] for b in batch]\n",
    "    ids = [b[\"id\"] for b in batch]\n",
    "\n",
    "    B = len(motions)\n",
    "    T_max = int(lengths.max())\n",
    "    Fdim = motions[0].shape[1]\n",
    "\n",
    "    padded = torch.zeros(B, T_max, Fdim)\n",
    "    mask = torch.zeros(B, T_max, dtype=torch.bool)\n",
    "\n",
    "    for i, m in enumerate(motions):\n",
    "        T = m.shape[0]\n",
    "        padded[i, :T] = m\n",
    "        mask[i, :T] = 1\n",
    "\n",
    "    return {\"motion\": padded, \"mask\": mask, \"lengths\": lengths, \"texts\": texts, \"ids\": ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89b10b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading preprocessed dataset from KIT-ML\\cached_train_maxlen196_normTrue.pt...\n",
      "Loaded 4886 preprocessed motions from cache\n"
     ]
    }
   ],
   "source": [
    "dataset = KITMotionDataset(\n",
    "    root=\"KIT-ML\",\n",
    "    split=\"train\",\n",
    "    max_len=196,\n",
    ")\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_motion,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "041f1b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timestep_embedding(timesteps: torch.Tensor, dim: int) -> torch.Tensor:\n",
    "    half = dim // 2\n",
    "    freqs = torch.exp(\n",
    "        -math.log(10000) * torch.arange(0, half, device=timesteps.device).float() / (half - 1)\n",
    "    )\n",
    "    args = timesteps.float().unsqueeze(1) * freqs.unsqueeze(0)\n",
    "    emb = torch.cat([torch.sin(args), torch.cos(args)], dim=1)\n",
    "    if dim % 2 == 1:\n",
    "        emb = F.pad(emb, (0, 1))\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c67f1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiffusionSchedule:\n",
    "    def __init__(self, T: int = 1000, beta_start=1e-4, beta_end=2e-2, device=\"cpu\"):\n",
    "        self.T = T\n",
    "        betas = torch.linspace(beta_start, beta_end, T, device=device)\n",
    "        alphas = 1.0 - betas\n",
    "        alpha_bar = torch.cumprod(alphas, dim=0)\n",
    "\n",
    "        self.betas = betas\n",
    "        self.alphas = alphas\n",
    "        self.alpha_bar = alpha_bar\n",
    "        self.sqrt_alpha_bar = torch.sqrt(alpha_bar)\n",
    "        self.sqrt_one_minus_alpha_bar = torch.sqrt(1.0 - alpha_bar)\n",
    "\n",
    "    def q_sample(self, x0, t, noise):\n",
    "        B = x0.shape[0]\n",
    "        # make (B, 1, 1, ..., 1) to broadcast over all non-batch dims\n",
    "        broadcast_shape = [B] + [1] * (x0.ndim - 1)\n",
    "        s1 = self.sqrt_alpha_bar[t].view(*broadcast_shape)\n",
    "        s2 = self.sqrt_one_minus_alpha_bar[t].view(*broadcast_shape)\n",
    "        return s1 * x0 + s2 * noise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "271de40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MotionDenoiserTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Drop-in replacement that fixes the key issue: adds FRAME positional embeddings\n",
    "    (sequence-time / frame index), in addition to your existing diffusion timestep embedding.\n",
    "\n",
    "    Same call signature:\n",
    "        eps_hat = model(x_t, t, text_emb, mask=mask)\n",
    "\n",
    "    Assumes mask is (B, T) with True=valid, False=padding.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_joints: int = 21,\n",
    "        coord_dim: int = 3,\n",
    "        d_model: int = 512,\n",
    "        n_layers: int = 8,\n",
    "        n_heads: int = 8,\n",
    "        dropout: float = 0.1,\n",
    "        time_embed_dim: int = 512,\n",
    "        clip_dim: int = 512,\n",
    "        max_frames: int = 256,   # <-- IMPORTANT: set >= your dataset max sequence length\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_joints = num_joints\n",
    "        self.coord_dim = coord_dim\n",
    "        self.d_model = d_model\n",
    "        self.max_frames = max_frames\n",
    "\n",
    "        in_dim = num_joints * coord_dim\n",
    "\n",
    "        # Frame encoder: (J*3) -> d_model\n",
    "        self.frame_in = nn.Sequential(\n",
    "            nn.Linear(in_dim, d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_model, d_model),\n",
    "        )\n",
    "\n",
    "        # Diffusion timestep embedding -> d_model\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            nn.Linear(time_embed_dim, d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_model, d_model),\n",
    "        )\n",
    "\n",
    "        # Text embedding projection -> d_model (single token)\n",
    "        self.text_proj = nn.Sequential(\n",
    "            nn.Linear(clip_dim, d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_model, d_model),\n",
    "        )\n",
    "\n",
    "        # --- NEW: learnable FRAME positional embedding (sequence-time) ---\n",
    "        # We prepend 1 text token, so allocate max_frames + 1 positions.\n",
    "        self.pos_emb = nn.Parameter(torch.zeros(1, max_frames + 1, d_model))\n",
    "        nn.init.normal_(self.pos_emb, std=0.02)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=d_model * 4,\n",
    "            dropout=dropout,\n",
    "            activation=\"gelu\",\n",
    "            batch_first=True,\n",
    "            norm_first=True,\n",
    "        )\n",
    "        self.tr = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "\n",
    "        # Decode back to (J*3)\n",
    "        self.frame_out = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_model, in_dim),\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        x_t: torch.Tensor,\n",
    "        t: torch.Tensor,\n",
    "        text_emb: torch.Tensor,\n",
    "        mask: torch.Tensor | None = None,\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x_t: (B, T, J, C)\n",
    "        t:   (B,) diffusion timestep indices\n",
    "        text_emb: (B, clip_dim)\n",
    "        mask: (B, T) True=valid, False=padding\n",
    "        \"\"\"\n",
    "        B, Tm, J, C = x_t.shape\n",
    "        assert J == self.num_joints and C == self.coord_dim\n",
    "\n",
    "        if Tm > self.max_frames:\n",
    "            raise ValueError(\n",
    "                f\"Sequence length T={Tm} exceeds max_frames={self.max_frames}. \"\n",
    "                f\"Increase max_frames in MotionDenoiserTransformer.\"\n",
    "            )\n",
    "\n",
    "        # Frame tokens\n",
    "        x = x_t.reshape(B, Tm, J * C)\n",
    "        h = self.frame_in(x)  # (B, T, d_model)\n",
    "\n",
    "        # Diffusion timestep embedding (same added to every frame token)\n",
    "        te = timestep_embedding(t, dim=self.time_mlp[0].in_features)  # (B, time_embed_dim)\n",
    "        te = self.time_mlp(te).unsqueeze(1)                           # (B, 1, d_model)\n",
    "        h = h + te                                                    # broadcast over frames\n",
    "\n",
    "        # Text token\n",
    "        text_tok = self.text_proj(text_emb).unsqueeze(1)              # (B, 1, d_model)\n",
    "\n",
    "        # Concatenate [text, frames]\n",
    "        h = torch.cat([text_tok, h], dim=1)                           # (B, 1+T, d_model)\n",
    "\n",
    "        # --- NEW: add positional embedding for (text + frames) ---\n",
    "        h = h + self.pos_emb[:, : (1 + Tm), :]\n",
    "\n",
    "        # Build key padding mask: PyTorch expects True where padding\n",
    "        if mask is not None:\n",
    "            mask = mask.to(device=x_t.device)\n",
    "            text_valid = torch.ones(B, 1, dtype=torch.bool, device=x_t.device)\n",
    "            full_valid = torch.cat([text_valid, mask], dim=1)         # (B, 1+T)\n",
    "            src_key_padding_mask = ~full_valid                        # True where padding\n",
    "        else:\n",
    "            src_key_padding_mask = None\n",
    "\n",
    "        # Transformer encode\n",
    "        h = self.tr(src=h, src_key_padding_mask=src_key_padding_mask)  # (B, 1+T, d_model)\n",
    "\n",
    "        # Drop text token\n",
    "        h = h[:, 1:, :]                                                # (B, T, d_model)\n",
    "\n",
    "        # Predict noise (or x0) in original motion shape\n",
    "        y = self.frame_out(h)                                          # (B, T, J*C)\n",
    "        return y.reshape(B, Tm, J, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2bd978e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MotionDDPM(nn.Module):\n",
    "    def __init__(self, denoiser: nn.Module, schedule):\n",
    "        super().__init__()\n",
    "        self.denoiser = denoiser\n",
    "        self.schedule = schedule\n",
    "\n",
    "    def training_step(self, x0: torch.Tensor, text_emb: torch.Tensor, mask: torch.Tensor | None = None):\n",
    "        B = x0.shape[0]\n",
    "        device = x0.device\n",
    "\n",
    "        t = torch.randint(0, self.schedule.T, (B,), device=device)\n",
    "        noise = torch.randn_like(x0)  # DO NOT mask noise here\n",
    "\n",
    "        x_t = self.schedule.q_sample(x0, t, noise)\n",
    "        eps_hat = self.denoiser(x_t, t, text_emb, mask=mask)\n",
    "\n",
    "        if mask is None:\n",
    "            return F.mse_loss(eps_hat, noise)\n",
    "\n",
    "        # mask is (B,T) with True=valid\n",
    "        m = mask.to(device=device).float()  # (B,T)\n",
    "        m = m.unsqueeze(-1).unsqueeze(-1)   # (B,T,1,1)\n",
    "\n",
    "        se = (eps_hat - noise) ** 2\n",
    "        se = se * m\n",
    "\n",
    "        denom = m.sum() * x0.shape[2] * x0.shape[3]  # valid frames * (J*C)\n",
    "        loss = se.sum() / denom.clamp(min=1e-8)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e621a152",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def ddpm_sample(\n",
    "    model,\n",
    "    shape,\n",
    "    text_emb: torch.Tensor,\n",
    "    mask: torch.Tensor | None = None,\n",
    "    device: str | torch.device = \"cpu\",\n",
    "):\n",
    "    B, Tm, J, C = shape\n",
    "    sched = model.schedule\n",
    "    device = torch.device(device)\n",
    "\n",
    "    # Move schedule tensors to the right device/dtype once\n",
    "    betas = sched.betas.to(device=device)\n",
    "    alphas = sched.alphas.to(device=device)\n",
    "    alpha_bar = sched.alpha_bar.to(device=device)\n",
    "\n",
    "    # Start from pure noise x_T\n",
    "    x = torch.randn(shape, device=device)\n",
    "\n",
    "    # Optional: keep padding untouched during the chain (recommended: don't hard-zero every step)\n",
    "    # We'll apply masking only at the end. Still pass mask into denoiser for attention masking.\n",
    "    mask_broadcast = None\n",
    "    if mask is not None:\n",
    "        # Expect mask True=valid; convert to float broadcast (B,T,1,1)\n",
    "        mask_broadcast = mask.to(device=device).unsqueeze(-1).unsqueeze(-1).float()\n",
    "\n",
    "    for ti in reversed(range(sched.T)):\n",
    "        t = torch.full((B,), ti, device=device, dtype=torch.long)\n",
    "\n",
    "        # Predict noise epsilon_hat\n",
    "        eps_hat = model.denoiser(x, t, text_emb.to(device=device), mask=mask.to(device=device) if mask is not None else None)\n",
    "\n",
    "        beta_t = betas[ti]\n",
    "        alpha_t = alphas[ti]\n",
    "        alpha_bar_t = alpha_bar[ti]\n",
    "\n",
    "        # DDPM mean: μθ(x_t,t) = 1/sqrt(α_t) * (x_t - (β_t / sqrt(1-ᾱ_t)) * εθ )\n",
    "        coef1 = torch.rsqrt(alpha_t)  # 1/sqrt(alpha_t)\n",
    "        coef2 = beta_t / torch.sqrt(torch.clamp(1.0 - alpha_bar_t, min=1e-12))\n",
    "        mean = coef1 * (x - coef2 * eps_hat)\n",
    "\n",
    "        if ti > 0:\n",
    "            # Posterior variance (tilde beta): β̃_t = β_t * (1-ᾱ_{t-1})/(1-ᾱ_t)\n",
    "            alpha_bar_prev = alpha_bar[ti - 1]\n",
    "            beta_tilde = beta_t * (1.0 - alpha_bar_prev) / torch.clamp(1.0 - alpha_bar_t, min=1e-12)\n",
    "            beta_tilde = torch.clamp(beta_tilde, min=1e-20)  # numerical safety\n",
    "\n",
    "            z = torch.randn_like(x)\n",
    "            x = mean + torch.sqrt(beta_tilde) * z\n",
    "        else:\n",
    "            x = mean\n",
    "\n",
    "    # Crop to actual lengths instead of masking\n",
    "    # Return list of tensors with different lengths\n",
    "    if mask is not None:\n",
    "        lengths = mask.sum(dim=1).cpu().long()  # (B,)\n",
    "        cropped = []\n",
    "        for i in range(B):\n",
    "            cropped.append(x[i, :lengths[i]])  # (T_i, J, C)\n",
    "        return cropped\n",
    "    else:\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17ba55b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "CLIP model loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Daniel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n",
      "C:\\Users\\Daniel\\AppData\\Local\\Temp\\ipykernel_13288\\3026012098.py:60: UserWarning: Converting a tensor with requires_grad=True to a scalar may lead to unexpected behavior.\n",
      "Consider using tensor.detach() first. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\torch\\csrc\\autograd\\generated\\python_variable_methods.cpp:837.)\n",
      "  print(f\"Epoch {epoch}, Step {global_step}, Loss: {float(loss):.4f}, LR: {sched.get_last_lr()[0]:.6e}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Step 0, Loss: 1.3912, LR: 2.000000e-04\n",
      "Epoch 0, Step 100, Loss: 0.3408, LR: 1.999786e-04\n",
      "Completed epoch 1/100\n",
      "Epoch 1, Step 200, Loss: 0.1287, LR: 1.999153e-04\n",
      "Epoch 1, Step 300, Loss: 0.1877, LR: 1.998100e-04\n",
      "Completed epoch 2/100\n",
      "Epoch 2, Step 400, Loss: 0.1249, LR: 1.996629e-04\n",
      "Completed epoch 3/100\n",
      "Epoch 3, Step 500, Loss: 0.0801, LR: 1.994740e-04\n",
      "Epoch 3, Step 600, Loss: 0.1950, LR: 1.992433e-04\n",
      "Completed epoch 4/100\n",
      "Epoch 4, Step 700, Loss: 0.1515, LR: 1.989710e-04\n",
      "Completed epoch 5/100\n",
      "Epoch 5, Step 800, Loss: 0.1019, LR: 1.986572e-04\n",
      "Epoch 5, Step 900, Loss: 0.0963, LR: 1.983021e-04\n",
      "Completed epoch 6/100\n",
      "Epoch 6, Step 1000, Loss: 0.1681, LR: 1.979057e-04\n",
      "  Generated sample shape: torch.Size([119, 21, 3])\n",
      "  Conditioned on: a person break dances\n",
      "  Saved to kit-ml-diffusion\\sample_step001000.npy\n",
      "Completed epoch 7/100\n",
      "Epoch 7, Step 1100, Loss: 0.0673, LR: 1.974682e-04\n",
      "Epoch 7, Step 1200, Loss: 0.1310, LR: 1.969898e-04\n",
      "Completed epoch 8/100\n",
      "Epoch 8, Step 1300, Loss: 0.0982, LR: 1.964708e-04\n",
      "Completed epoch 9/100\n",
      "Epoch 9, Step 1400, Loss: 0.0849, LR: 1.959113e-04\n",
      "Epoch 9, Step 1500, Loss: 0.0831, LR: 1.953115e-04\n",
      "Completed epoch 10/100\n",
      "Epoch 10, Step 1600, Loss: 0.1317, LR: 1.946718e-04\n",
      "Completed epoch 11/100\n",
      "Epoch 11, Step 1700, Loss: 0.0893, LR: 1.939924e-04\n",
      "Epoch 11, Step 1800, Loss: 0.0490, LR: 1.932736e-04\n",
      "Completed epoch 12/100\n",
      "Epoch 12, Step 1900, Loss: 0.1249, LR: 1.925157e-04\n",
      "Completed epoch 13/100\n",
      "Epoch 13, Step 2000, Loss: 0.0637, LR: 1.917189e-04\n",
      "  Generated sample shape: torch.Size([71, 21, 3])\n",
      "  Conditioned on: A person walks a curve 90 degree to the left.\n",
      "  Saved to kit-ml-diffusion\\sample_step002000.npy\n",
      "Epoch 13, Step 2100, Loss: 0.0723, LR: 1.908838e-04\n",
      "Completed epoch 14/100\n",
      "Epoch 14, Step 2200, Loss: 0.0612, LR: 1.900105e-04\n",
      "Completed epoch 15/100\n",
      "Epoch 15, Step 2300, Loss: 0.1573, LR: 1.890994e-04\n",
      "Epoch 15, Step 2400, Loss: 0.1036, LR: 1.881510e-04\n",
      "Completed epoch 16/100\n",
      "Epoch 16, Step 2500, Loss: 0.0831, LR: 1.871657e-04\n",
      "Epoch 16, Step 2600, Loss: 0.0414, LR: 1.861438e-04\n",
      "Completed epoch 17/100\n",
      "Epoch 17, Step 2700, Loss: 0.0792, LR: 1.850858e-04\n",
      "Completed epoch 18/100\n",
      "Epoch 18, Step 2800, Loss: 0.0861, LR: 1.839922e-04\n",
      "Epoch 18, Step 2900, Loss: 0.0558, LR: 1.828634e-04\n",
      "Completed epoch 19/100\n",
      "Epoch 19, Step 3000, Loss: 0.0951, LR: 1.816998e-04\n",
      "  Generated sample shape: torch.Size([40, 21, 3])\n",
      "  Conditioned on: A person is sitting on a chair and than getting up from the chair.\n",
      "  Saved to kit-ml-diffusion\\sample_step003000.npy\n",
      "Completed epoch 20/100\n",
      "Epoch 20, Step 3100, Loss: 0.0973, LR: 1.805020e-04\n",
      "Epoch 20, Step 3200, Loss: 0.0841, LR: 1.792704e-04\n",
      "Completed epoch 21/100\n",
      "Epoch 21, Step 3300, Loss: 0.0498, LR: 1.780057e-04\n",
      "Completed epoch 22/100\n",
      "Epoch 22, Step 3400, Loss: 0.0856, LR: 1.767083e-04\n",
      "Epoch 22, Step 3500, Loss: 0.0476, LR: 1.753787e-04\n",
      "Completed epoch 23/100\n",
      "Epoch 23, Step 3600, Loss: 0.0671, LR: 1.740176e-04\n",
      "Completed epoch 24/100\n",
      "Epoch 24, Step 3700, Loss: 0.0753, LR: 1.726255e-04\n",
      "Epoch 24, Step 3800, Loss: 0.0596, LR: 1.712030e-04\n",
      "Completed epoch 25/100\n",
      "Epoch 25, Step 3900, Loss: 0.0366, LR: 1.697507e-04\n",
      "Completed epoch 26/100\n",
      "Epoch 26, Step 4000, Loss: 0.0344, LR: 1.682691e-04\n",
      "  Generated sample shape: torch.Size([54, 21, 3])\n",
      "  Conditioned on: A human plays golf with a careful movement.\n",
      "  Saved to kit-ml-diffusion\\sample_step004000.npy\n",
      "Epoch 26, Step 4100, Loss: 0.0574, LR: 1.667590e-04\n",
      "Completed epoch 27/100\n",
      "Epoch 27, Step 4200, Loss: 0.0718, LR: 1.652210e-04\n",
      "Completed epoch 28/100\n",
      "Epoch 28, Step 4300, Loss: 0.0610, LR: 1.636557e-04\n",
      "Epoch 28, Step 4400, Loss: 0.0730, LR: 1.620637e-04\n",
      "Completed epoch 29/100\n",
      "Epoch 29, Step 4500, Loss: 0.0548, LR: 1.604458e-04\n",
      "Completed epoch 30/100\n",
      "Epoch 30, Step 4600, Loss: 0.0474, LR: 1.588027e-04\n",
      "Epoch 30, Step 4700, Loss: 0.0489, LR: 1.571349e-04\n",
      "Completed epoch 31/100\n",
      "Epoch 31, Step 4800, Loss: 0.0636, LR: 1.554433e-04\n",
      "Completed epoch 32/100\n",
      "Epoch 32, Step 4900, Loss: 0.0706, LR: 1.537285e-04\n",
      "Epoch 32, Step 5000, Loss: 0.0419, LR: 1.519913e-04\n",
      "  Generated sample shape: torch.Size([196, 21, 3])\n",
      "  Conditioned on: A person exercises\n",
      "  Saved to kit-ml-diffusion\\sample_step005000.npy\n",
      "Completed epoch 33/100\n",
      "Epoch 33, Step 5100, Loss: 0.0491, LR: 1.502323e-04\n",
      "Epoch 33, Step 5200, Loss: 0.0839, LR: 1.484524e-04\n",
      "Completed epoch 34/100\n",
      "Epoch 34, Step 5300, Loss: 0.0490, LR: 1.466523e-04\n",
      "Completed epoch 35/100\n",
      "Epoch 35, Step 5400, Loss: 0.0616, LR: 1.448327e-04\n",
      "Epoch 35, Step 5500, Loss: 0.0690, LR: 1.429944e-04\n",
      "Completed epoch 36/100\n",
      "Epoch 36, Step 5600, Loss: 0.0485, LR: 1.411382e-04\n",
      "Completed epoch 37/100\n",
      "Epoch 37, Step 5700, Loss: 0.0482, LR: 1.392649e-04\n",
      "Epoch 37, Step 5800, Loss: 0.0696, LR: 1.373752e-04\n",
      "Completed epoch 38/100\n",
      "Epoch 38, Step 5900, Loss: 0.0526, LR: 1.354700e-04\n",
      "Completed epoch 39/100\n",
      "Epoch 39, Step 6000, Loss: 0.0563, LR: 1.335500e-04\n",
      "  Generated sample shape: torch.Size([55, 21, 3])\n",
      "  Conditioned on: Person stands up from a kneeing position.\n",
      "  Saved to kit-ml-diffusion\\sample_step006000.npy\n",
      "Epoch 39, Step 6100, Loss: 0.0254, LR: 1.316161e-04\n",
      "Completed epoch 40/100\n",
      "Epoch 40, Step 6200, Loss: 0.0990, LR: 1.296691e-04\n",
      "Completed epoch 41/100\n",
      "Epoch 41, Step 6300, Loss: 0.0493, LR: 1.277098e-04\n",
      "Epoch 41, Step 6400, Loss: 0.0871, LR: 1.257390e-04\n",
      "Completed epoch 42/100\n",
      "Epoch 42, Step 6500, Loss: 0.0288, LR: 1.237576e-04\n",
      "Completed epoch 43/100\n",
      "Epoch 43, Step 6600, Loss: 0.0553, LR: 1.217664e-04\n",
      "Epoch 43, Step 6700, Loss: 0.1046, LR: 1.197662e-04\n",
      "Completed epoch 44/100\n",
      "Epoch 44, Step 6800, Loss: 0.0325, LR: 1.177579e-04\n",
      "Completed epoch 45/100\n",
      "Epoch 45, Step 6900, Loss: 0.0481, LR: 1.157423e-04\n",
      "Epoch 45, Step 7000, Loss: 0.0409, LR: 1.137203e-04\n",
      "  Generated sample shape: torch.Size([70, 21, 3])\n",
      "  Conditioned on: Some walks a distance of quarter of a circle to the right.\n",
      "  Saved to kit-ml-diffusion\\sample_step007000.npy\n",
      "Completed epoch 46/100\n",
      "Epoch 46, Step 7100, Loss: 0.0364, LR: 1.116927e-04\n",
      "Completed epoch 47/100\n",
      "Epoch 47, Step 7200, Loss: 0.0316, LR: 1.096604e-04\n",
      "Epoch 47, Step 7300, Loss: 0.0433, LR: 1.076242e-04\n",
      "Completed epoch 48/100\n",
      "Epoch 48, Step 7400, Loss: 0.0548, LR: 1.055850e-04\n",
      "Completed epoch 49/100\n",
      "Epoch 49, Step 7500, Loss: 0.0622, LR: 1.035437e-04\n",
      "Epoch 49, Step 7600, Loss: 0.0403, LR: 1.015011e-04\n",
      "Completed epoch 50/100\n",
      "Epoch 50, Step 7700, Loss: 0.0561, LR: 9.945806e-05\n",
      "Epoch 50, Step 7800, Loss: 0.0511, LR: 9.741547e-05\n",
      "Completed epoch 51/100\n",
      "Epoch 51, Step 7900, Loss: 0.0634, LR: 9.537418e-05\n",
      "Completed epoch 52/100\n",
      "Epoch 52, Step 8000, Loss: 0.0294, LR: 9.333506e-05\n",
      "  Generated sample shape: torch.Size([70, 21, 3])\n",
      "  Conditioned on: Some walks a distance of quarter of a circle to the right.\n",
      "  Saved to kit-ml-diffusion\\sample_step008000.npy\n",
      "Epoch 52, Step 8100, Loss: 0.0667, LR: 9.129895e-05\n",
      "Completed epoch 53/100\n",
      "Epoch 53, Step 8200, Loss: 0.0408, LR: 8.926673e-05\n",
      "Completed epoch 54/100\n",
      "Epoch 54, Step 8300, Loss: 0.0558, LR: 8.723924e-05\n",
      "Epoch 54, Step 8400, Loss: 0.0591, LR: 8.521734e-05\n",
      "Completed epoch 55/100\n",
      "Epoch 55, Step 8500, Loss: 0.0546, LR: 8.320189e-05\n",
      "Completed epoch 56/100\n",
      "Epoch 56, Step 8600, Loss: 0.0565, LR: 8.119372e-05\n",
      "Epoch 56, Step 8700, Loss: 0.0604, LR: 7.919370e-05\n",
      "Completed epoch 57/100\n",
      "Epoch 57, Step 8800, Loss: 0.0495, LR: 7.720266e-05\n",
      "Completed epoch 58/100\n",
      "Epoch 58, Step 8900, Loss: 0.0559, LR: 7.522144e-05\n",
      "Epoch 58, Step 9000, Loss: 0.0520, LR: 7.325088e-05\n",
      "  Generated sample shape: torch.Size([124, 21, 3])\n",
      "  Conditioned on: A person is jumping forward with both feet.\n",
      "  Saved to kit-ml-diffusion\\sample_step009000.npy\n",
      "Completed epoch 59/100\n",
      "Epoch 59, Step 9100, Loss: 0.0470, LR: 7.129181e-05\n",
      "Completed epoch 60/100\n",
      "Epoch 60, Step 9200, Loss: 0.0567, LR: 6.934505e-05\n",
      "Epoch 60, Step 9300, Loss: 0.0738, LR: 6.741143e-05\n",
      "Completed epoch 61/100\n",
      "Epoch 61, Step 9400, Loss: 0.0465, LR: 6.549176e-05\n",
      "Completed epoch 62/100\n",
      "Epoch 62, Step 9500, Loss: 0.0335, LR: 6.358684e-05\n",
      "Epoch 62, Step 9600, Loss: 0.0906, LR: 6.169749e-05\n",
      "Completed epoch 63/100\n",
      "Epoch 63, Step 9700, Loss: 0.0447, LR: 5.982450e-05\n",
      "Completed epoch 64/100\n",
      "Epoch 64, Step 9800, Loss: 0.0833, LR: 5.796866e-05\n",
      "Epoch 64, Step 9900, Loss: 0.0613, LR: 5.613075e-05\n",
      "Completed epoch 65/100\n",
      "Epoch 65, Step 10000, Loss: 0.0741, LR: 5.431154e-05\n",
      "  Generated sample shape: torch.Size([196, 21, 3])\n",
      "  Conditioned on: a person walks in random curves\n",
      "  Saved to kit-ml-diffusion\\sample_step010000.npy\n",
      "Completed epoch 66/100\n",
      "Epoch 66, Step 10100, Loss: 0.0523, LR: 5.251181e-05\n",
      "Epoch 66, Step 10200, Loss: 0.0330, LR: 5.073231e-05\n",
      "Completed epoch 67/100\n",
      "Epoch 67, Step 10300, Loss: 0.0311, LR: 4.897379e-05\n",
      "Epoch 67, Step 10400, Loss: 0.0429, LR: 4.723699e-05\n",
      "Completed epoch 68/100\n",
      "Epoch 68, Step 10500, Loss: 0.0267, LR: 4.552266e-05\n",
      "Completed epoch 69/100\n",
      "Epoch 69, Step 10600, Loss: 0.0284, LR: 4.383149e-05\n",
      "Epoch 69, Step 10700, Loss: 0.0387, LR: 4.216423e-05\n",
      "Completed epoch 70/100\n",
      "Epoch 70, Step 10800, Loss: 0.0554, LR: 4.052155e-05\n",
      "Completed epoch 71/100\n",
      "Epoch 71, Step 10900, Loss: 0.0514, LR: 3.890416e-05\n",
      "Epoch 71, Step 11000, Loss: 0.0206, LR: 3.731274e-05\n",
      "  Generated sample shape: torch.Size([62, 21, 3])\n",
      "  Conditioned on: A person performs a kicking motion.\n",
      "  Saved to kit-ml-diffusion\\sample_step011000.npy\n",
      "Completed epoch 72/100\n",
      "Epoch 72, Step 11100, Loss: 0.0585, LR: 3.574797e-05\n",
      "Completed epoch 73/100\n",
      "Epoch 73, Step 11200, Loss: 0.0765, LR: 3.421049e-05\n",
      "Epoch 73, Step 11300, Loss: 0.0357, LR: 3.270095e-05\n",
      "Completed epoch 74/100\n",
      "Epoch 74, Step 11400, Loss: 0.0402, LR: 3.122001e-05\n",
      "Completed epoch 75/100\n",
      "Epoch 75, Step 11500, Loss: 0.0627, LR: 2.976827e-05\n",
      "Epoch 75, Step 11600, Loss: 0.0418, LR: 2.834635e-05\n",
      "Completed epoch 76/100\n",
      "Epoch 76, Step 11700, Loss: 0.0525, LR: 2.695485e-05\n",
      "Completed epoch 77/100\n",
      "Epoch 77, Step 11800, Loss: 0.0365, LR: 2.559436e-05\n",
      "Epoch 77, Step 11900, Loss: 0.0559, LR: 2.426545e-05\n",
      "Completed epoch 78/100\n",
      "Epoch 78, Step 12000, Loss: 0.0311, LR: 2.296868e-05\n",
      "  Generated sample shape: torch.Size([72, 21, 3])\n",
      "  Conditioned on: A person is absorbing a puch with the left hand while holding the hands up.\n",
      "  Saved to kit-ml-diffusion\\sample_step012000.npy\n",
      "Completed epoch 79/100\n",
      "Epoch 79, Step 12100, Loss: 0.0397, LR: 2.170459e-05\n",
      "Epoch 79, Step 12200, Loss: 0.0841, LR: 2.047373e-05\n",
      "Completed epoch 80/100\n",
      "Epoch 80, Step 12300, Loss: 0.0489, LR: 1.927660e-05\n",
      "Completed epoch 81/100\n",
      "Epoch 81, Step 12400, Loss: 0.0389, LR: 1.811372e-05\n",
      "Epoch 81, Step 12500, Loss: 0.0229, LR: 1.698558e-05\n",
      "Completed epoch 82/100\n",
      "Epoch 82, Step 12600, Loss: 0.0478, LR: 1.589264e-05\n",
      "Completed epoch 83/100\n",
      "Epoch 83, Step 12700, Loss: 0.0240, LR: 1.483538e-05\n",
      "Epoch 83, Step 12800, Loss: 0.0568, LR: 1.381423e-05\n",
      "Completed epoch 84/100\n",
      "Epoch 84, Step 12900, Loss: 0.0342, LR: 1.282962e-05\n",
      "Epoch 84, Step 13000, Loss: 0.0257, LR: 1.188198e-05\n",
      "  Generated sample shape: torch.Size([33, 21, 3])\n",
      "  Conditioned on: A person throws with their right hand\n",
      "  Saved to kit-ml-diffusion\\sample_step013000.npy\n",
      "Completed epoch 85/100\n",
      "Epoch 85, Step 13100, Loss: 0.0430, LR: 1.097170e-05\n",
      "Completed epoch 86/100\n",
      "Epoch 86, Step 13200, Loss: 0.0773, LR: 1.009917e-05\n",
      "Epoch 86, Step 13300, Loss: 0.0391, LR: 9.264747e-06\n",
      "Completed epoch 87/100\n",
      "Epoch 87, Step 13400, Loss: 0.0387, LR: 8.468791e-06\n",
      "Completed epoch 88/100\n",
      "Epoch 88, Step 13500, Loss: 0.0322, LR: 7.711635e-06\n",
      "Epoch 88, Step 13600, Loss: 0.0828, LR: 6.993599e-06\n",
      "Completed epoch 89/100\n",
      "Epoch 89, Step 13700, Loss: 0.0307, LR: 6.314985e-06\n",
      "Completed epoch 90/100\n",
      "Epoch 90, Step 13800, Loss: 0.0732, LR: 5.676079e-06\n",
      "Epoch 90, Step 13900, Loss: 0.0412, LR: 5.077152e-06\n",
      "Completed epoch 91/100\n",
      "Epoch 91, Step 14000, Loss: 0.0818, LR: 4.518455e-06\n",
      "  Generated sample shape: torch.Size([65, 21, 3])\n",
      "  Conditioned on: A human jumps forwards.\n",
      "  Saved to kit-ml-diffusion\\sample_step014000.npy\n",
      "Completed epoch 92/100\n",
      "Epoch 92, Step 14100, Loss: 0.0469, LR: 4.000224e-06\n",
      "Epoch 92, Step 14200, Loss: 0.0260, LR: 3.522677e-06\n",
      "Completed epoch 93/100\n",
      "Epoch 93, Step 14300, Loss: 0.1020, LR: 3.086016e-06\n",
      "Completed epoch 94/100\n",
      "Epoch 94, Step 14400, Loss: 0.0490, LR: 2.690425e-06\n",
      "Epoch 94, Step 14500, Loss: 0.0411, LR: 2.336070e-06\n",
      "Completed epoch 95/100\n",
      "Epoch 95, Step 14600, Loss: 0.0684, LR: 2.023101e-06\n",
      "Completed epoch 96/100\n",
      "Epoch 96, Step 14700, Loss: 0.0395, LR: 1.751651e-06\n",
      "Epoch 96, Step 14800, Loss: 0.0386, LR: 1.521832e-06\n",
      "Completed epoch 97/100\n",
      "Epoch 97, Step 14900, Loss: 0.0407, LR: 1.333744e-06\n",
      "Completed epoch 98/100\n",
      "Epoch 98, Step 15000, Loss: 0.0881, LR: 1.187463e-06\n",
      "  Generated sample shape: torch.Size([76, 21, 3])\n",
      "  Conditioned on: A person laying on their hands and knees stands up by using their left foot first, then pushing off it to stay balanced.\n",
      "  Saved to kit-ml-diffusion\\sample_step015000.npy\n",
      "Epoch 98, Step 15100, Loss: 0.0635, LR: 1.083053e-06\n",
      "Completed epoch 99/100\n",
      "Epoch 99, Step 15200, Loss: 0.0430, LR: 1.020557e-06\n",
      "Completed epoch 100/100\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "import ssl\n",
    "\n",
    "from matplotlib.pylab import std\n",
    "from numpy import mean\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load CLIP model\n",
    "clip_model, clip_preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "clip_model.eval()  # Freeze CLIP\n",
    "for param in clip_model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(\"CLIP model loaded\")\n",
    "\n",
    "num_joints = 21\n",
    "denoiser = MotionDenoiserTransformer(\n",
    "    num_joints=num_joints, \n",
    "    d_model=512, \n",
    "    n_layers=8, \n",
    "    n_heads=8,\n",
    "    clip_dim=512  # CLIP ViT-B/32 embedding dimension\n",
    ").to(device)\n",
    "\n",
    "schedule = DiffusionSchedule(T=1000, device=device)\n",
    "ddpm = MotionDDPM(denoiser, schedule).to(device)\n",
    "\n",
    "opt = torch.optim.AdamW(ddpm.parameters(), lr=2e-4, weight_decay=1e-4)\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "sched = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=num_epochs * len(loader), eta_min=1e-6)\n",
    "\n",
    "# Create output directory for samples\n",
    "output_dir = \"kit-ml-diffusion\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "global_step = 0\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in loader:\n",
    "        x0 = batch[\"motion\"].to(device)     # (B,T,J,3)\n",
    "        mask = batch[\"mask\"].to(device)     # (B,T)\n",
    "        texts = batch[\"texts\"]              # List of strings\n",
    "\n",
    "        # Encode text with CLIP\n",
    "        text_tokens = clip.tokenize(texts, truncate=True).to(device)\n",
    "        with torch.no_grad():\n",
    "            text_emb = clip_model.encode_text(text_tokens).float()  # (B, 512)\n",
    "            text_emb = text_emb / text_emb.norm(dim=-1, keepdim=True)\n",
    "        loss = ddpm.training_step(x0, text_emb, mask=mask)\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(ddpm.parameters(), 1.0)\n",
    "        opt.step()\n",
    "        sched.step()\n",
    "        \n",
    "        if global_step % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Step {global_step}, Loss: {float(loss):.4f}, LR: {sched.get_last_lr()[0]:.6e}\")\n",
    "            \n",
    "        # Sample occasionally and save\n",
    "        if global_step % 1000 == 0 and global_step > 0:\n",
    "            # Use text from first sample in batch for generation\n",
    "            samp_list = ddpm_sample(ddpm, x0.shape, text_emb, mask=mask, device=device)\n",
    "            \n",
    "            # Get first sample (now a cropped tensor)\n",
    "            samp = samp_list[0]  # (T_actual, J, C)\n",
    "            \n",
    "            # Denormalize using dataset statistics\n",
    "            if dataset.mean is not None and dataset.std is not None:\n",
    "                mean_ = dataset.mean.to(samp.device, samp.dtype)\n",
    "                std_ = dataset.std.to(samp.device, samp.dtype)\n",
    "                samp = samp * std_ + mean_\n",
    "\n",
    "            print(f\"  Generated sample shape: {samp.shape}\")\n",
    "            print(f\"  Conditioned on: {texts[0]}\")\n",
    "            \n",
    "            # Save first sample from batch\n",
    "            motion_np = samp.cpu().numpy()  # (T_actual, J, 3) - no padding!\n",
    "            save_path = os.path.join(output_dir, f\"sample_step{global_step:06d}.npy\")\n",
    "            np.save(save_path, motion_np)\n",
    "            \n",
    "            # Save text prompt\n",
    "            text_path = os.path.join(output_dir, f\"sample_step{global_step:06d}.txt\")\n",
    "            with open(text_path, \"w\") as f:\n",
    "                f.write(texts[0])\n",
    "            \n",
    "            print(f\"  Saved to {save_path}\")\n",
    "            \n",
    "        global_step += 1\n",
    "    \n",
    "    print(f\"Completed epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "print(\"Training complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
